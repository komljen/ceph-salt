{% import 'ceph/global_vars.jinja' as conf with context %}
[global]
    fsid = {{ salt['pillar.get']('ceph:global:fsid') }}
    public network = {{ salt['pillar.get']('ceph:global:public_network') }}
    cluster network = {{ salt['pillar.get']('ceph:global:cluster_network') }}
    auth cluster required = cephx
    auth service required = cephx
    auth client required = cephx

[osd]
    osd journal size = {{ salt['pillar.get']('ceph:osd:journal_size') }}
    osd pool default size = {{ salt['pillar.get']('ceph:osd:pool_default_size') }}
    osd pool default min size = {{ salt['pillar.get']('ceph:osd:pool_default_min_size') }}
    osd pool default pg num = {{ salt['pillar.get']('ceph:osd:pool_default_pg_num') }}
    osd pool default pgp num = {{ salt['pillar.get']('ceph:osd:pool_default_pgp_num') }}
    osd crush chooseleaf type = {{ salt['pillar.get']('ceph:osd:crush_chooseleaf_type') }}
    osd crush update on start = {{ salt['pillar.get']('ceph:osd:crush_update_on_start') }}
    filestore merge threshold = {{ salt['pillar.get']('ceph:osd:filestore_merge_threshold') }}
    filestore split multiple = {{ salt['pillar.get']('ceph:osd:filestore_split_multiple') }}
    filestore op threads = {{ salt['pillar.get']('ceph:osd:filestore_op_threads') }}
    filestore max sync interval = {{ salt['pillar.get']('ceph:osd:filestore_max_sync_interval') }}
    filestore min sync interval = {{ salt['pillar.get']('ceph:osd:filestore_min_sync_interval') }}
    filestore queue max ops = {{ salt['pillar.get']('ceph:osd:filestore_queue_max_ops') }}
    filestore queue max bytes = {{ salt['pillar.get']('ceph:osd:filestore_queue_max_bytes') }}
    filestore queue committing max ops = {{ salt['pillar.get']('ceph:osd:filestore_queue_committing_max_ops') }}
    filestore queue committing max bytes = {{ salt['pillar.get']('ceph:osd:filestore_queue_committing_max_bytes') }}
    osd op threads = {{ salt['pillar.get']('ceph:osd:op_threads') }}
    osd disk threads = {{ salt['pillar.get']('ceph:osd:disk_threads') }}
    osd max backfills = {{ salt['pillar.get']('ceph:osd:max_backfills') }}
    osd map cache size = {{ salt['pillar.get']('ceph:osd:map_cache_size') }}
    osd scrub load threshold = {{ salt['pillar.get']('ceph:osd:scrub_load_threshold') }}

[client.admin]
    keyring = /etc/ceph/{{ conf.cluster }}.client.admin.keyring
    public addr = 0.0.0.0:{{ salt['pillar.get']('ceph:rest_api:port', 5000) }}

[client]
    rbd cache = {{ salt['pillar.get']('ceph:client:rbd_cache') }}
    rbd cache writethrough until flush = {{ salt['pillar.get']('ceph:client:rbd_cache_writethrough_until_flush') }}
    rbd cache size = {{ salt['pillar.get']('ceph:client:rbd_cache_size') }}
    admin socket = {{ salt['pillar.get']('ceph:client:admin_socket') }}
    log file = {{ salt['pillar.get']('ceph:client:log_file') }}

{% for mon, grains in salt['mine.get']('environment:'+ conf.environment,'grains.items','grain').items() | sort %}
{% for role in grains.roles if role == 'ceph-mon' %}
[mon.{{ grains.host }}]
    mon host = {{ grains.host }}
    mon addr = {{ grains.ip_interfaces[conf.mon_interface][0] }}:6789

{% endfor %}
{% endfor %}
{% for mds, grains in salt['mine.get']('environment:'+ conf.environment,'grains.items','grain').items() | sort %}
{% for role in grains.roles if role == 'ceph-mds' %}
[mds.{{ grains.host }}]
    mds host = {{ grains.host }}

{% endfor %}
{% endfor %}
